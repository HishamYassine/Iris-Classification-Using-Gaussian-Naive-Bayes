# -*- coding: utf-8 -*-
"""Algorithm Selection - Gaussian NB.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HNskHrcSm3G7MGTE20iKeNJFti9lbxlq

# Challenge of the Week: Gaussian Naive Bayes Classifier
---
Â© 2022 Zaka AI, Inc. All Rights Reserved.

##**Case Study:** Iris Dataset

**Objective:** The objective of this challenge is to make you know about Naive Bayes applied on Numerical Values.

**DataSet Columns:**<br>
*	 Petal Height
*  Petal Width	
*  Sepal Height	
*  Sepal Width	
*  Target: The kind of the Iris flower (Virginica, Setosa, Versicolor)

# Importing Libraries

Start by importing the necessary libraries. For this problem we need the following:


*   Numpy: for numerical calculations
*   Pandas: to deal with the dataset
*   math: to work on the mathematical aspects of Naive Bayes
"""

import numpy as np
import pandas as pd
from math import sqrt

"""# Loading the Dataset

Load the dataset in your environment. One thing to note is that the dataset you have does not include names for different columns. This is why you should name the columns by hand as ['Sepal Height', 'Sepal Width', 'Petal Height', 'Petal Width', 'Target']. Then don't forget to show the head of your dataset to get a better insight into it.
"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Zaka - AIC/Week 3/iris.csv')
df.columns = ['Sepal Height', 'Sepal Width', 'Petal Height', 'Petal Width', 'Target']

df.head()

"""##Data Preprocessing

You may have noticed that the Target Column contains string values rather than numbers. This is why, you will Change the string values to numerical.
"""

from sklearn import preprocessing

encoder = preprocessing.LabelEncoder()
df['Target'] = encoder.fit_transform(df['Target'])
df.head()

"""Make sure we have no null values, and if we have, remove them."""

df.info()
print("We have 150 samples with no null values")

"""#Naive Bayes

##Finding different Classes

First, find how many classes we have in our dataset (although it should always appear in the description of your dataset)
"""

df['Target'].nunique()

"""SO we have 3 classes of flowers.

Remember the basic formula that we used for Naive Bayes. <br>
<img src="https://equatio-api.texthelp.com/svg/%5C%20P(%5Ctextcolor%7B%232B7FBB%7D%7BClass%7D%7C%5Ctextcolor%7B%23E94D40%7D%7BFeatures%7D)%3D%5Cfrac%7BP(%5Ctextcolor%7B%23E94D40%7D%7BFeatures%7D%7C%5Ctextcolor%7B%232B7FBB%7D%7BClass%7D)%5Ccdot%20P%5Cleft(%5Ctextcolor%7B%232B7FBB%7D%7BClass%7D%5Cright)%7D%7BP(%5Ctextcolor%7B%23E94D40%7D%7BFeatures%7D)%7D" alt="P of open paren C l a. s s divides F of e a. t u r e s close paren equals the fraction with numerator P of open paren F of e a. t u r e s divides C l a. s s close paren times P of open paren C l a. s s close paren and denominator P of F of e a. t u r e s">

Since we have 3 classes, and 4 features, we need to calculate the following probabilities.<br>
<img src="https://equatio-api.texthelp.com/svg/P(%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%7C%5Ctextcolor%7B%23E94D40%7D%7BF1%2CF2%2CF3%2CF4%7D)" alt="P of open paren C l a. s s sub 0 divides F of 1 comma F of 2 comma F of 3 comma F of 4 close paren"> <br>
<img src="https://equatio-api.texthelp.com/svg/P(%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%7C%5Ctextcolor%7B%23E94D40%7D%7BF1%2CF2%2CF3%2CF4%7D)" alt="P of open paren C l a. s s sub 1 divides F of 1 comma F of 2 comma F of 3 comma F of 4 close paren"> <br>
<img src="https://equatio-api.texthelp.com/svg/P(%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%7C%5Ctextcolor%7B%23E94D40%7D%7BF1%2CF2%2CF3%2CF4%7D)" alt="P of open paren C l a. s s sub 2 divides F of 1 comma F of 2 comma F of 3 comma F of 4 close paren">

So in reality we need to calculate the following:

<img src="https://equatio-api.texthelp.com/svg/P_0%3DP(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_1%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_2%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_3%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_4%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%7D)" alt="P sub 0 equals P of open paren F sub 1 divides C l a. s s sub 0 close paren P of open paren F sub 2 divides C l a. s s sub 0 close paren P of open paren F sub 3 divides C l a. s s sub 0 close paren P of open paren F sub 4 divides C l a. s s sub 0 close paren"><img src="https://equatio-api.texthelp.com/svg/P%5Cleft(%5Ctextcolor%7B%232B7FBB%7D%7BClass_0%7D%5Cright)" alt="P of open paren C l a. s s sub 0 close paren"><br><img src="https://equatio-api.texthelp.com/svg/P_1%3DP(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_1%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_2%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_3%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_4%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%7D)" alt="P sub 1 equals P of open paren F sub 1 divides C l a. s s sub 1 close paren P of open paren F sub 2 divides C l a. s s sub 1 close paren P of open paren F sub 3 divides C l a. s s sub 1 close paren P of open paren F sub 4 divides C l a. s s sub 1 close paren"><img src="https://equatio-api.texthelp.com/svg/P%5Cleft(%5Ctextcolor%7B%232B7FBB%7D%7BClass_1%7D%5Cright)" alt="P of open paren C l a. s s sub 1 close paren"><br>
<img src="https://equatio-api.texthelp.com/svg/P_2%3DP(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_1%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_2%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_3%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%7D)P(%5Ctextcolor%7B%232B7FBB%7D%7B%5Ctextcolor%7B%23E94D40%7D%7BF_4%7D%7D%7C%5Ctextcolor%7B%23E94D40%7D%7B%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%7D)P%5Cleft(%5Ctextcolor%7B%232B7FBB%7D%7BClass_2%7D%5Cright)" alt="P sub 2 equals P of open paren F sub 1 divides C l a. s s sub 2 close paren P of open paren F sub 2 divides C l a. s s sub 2 close paren P of open paren F sub 3 divides C l a. s s sub 2 close paren P of open paren F sub 4 divides C l a. s s sub 2 close paren P of open paren C l a. s s sub 2 close paren">

We see which one is the greatest, and based on that we assign the class.

Those probabilities will be approximated using a distribution. 
In this example, we will use the Gaussien Distribution.

##Gaussian Probability Density Function

We recall that teh Gaussien Probability density function is given by:
<br>
<img src="https://equatio-api.texthelp.com/svg/f%5Cleft(x%5Cright)%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Ctextcolor%7B%238D44AD%7D%7B%5Csigma%7D%7D%5Cexp%5Cleft%5C%7B-%5Cfrac%7B%5Cleft(x-%5Ctextcolor%7B%233697DC%7D%7Bmean%7D%5Cright)%5E2%7D%7B2%5Ctextcolor%7B%238D44AD%7D%7B%5Csigma%7D%5E2%7D%5Cright%5C%7D" alt="f of x equals 1 over the square root of 2 pi sigma the exp of open brace negative the fraction with numerator open paren x minus m e a. n close paren squared and denominator 2 sigma squared close brace">

Write a function that computes the probability using the formula above
"""

from math import exp
from math import pi
from math import sqrt

def Gaussian_pdf(x, mean, variance):
 float(mean)
 float(variance)
 gpdf=1/(sqrt(variance*2*pi)) * exp((- ((x-mean)**2))/(2*variance))

 return gpdf

"""##Naive Bayes Implementation

Write a naive bayes function that receives as input the dataframe df, the features, and the target name, and it returns the predicted class as output.
"""

def naive_bayes (df, features, target_name):

  target_0 = df[df[target_name] == 0]
  target_1 = df[df[target_name] == 1]
  target_2 = df[df[target_name] == 2]

  mean_sh_target_0 = target_0['Sepal Height'].mean() 
  mean_sw_target_0 = target_0['Sepal Width'].mean()
  mean_ph_target_0 = target_0['Petal Height'].mean()
  mean_pw_target_0 = target_0['Petal Width'].mean()

  var_sh_target_0 = target_0['Sepal Height'].var() 
  var_sw_target_0 = target_0['Sepal Width'].var()
  var_ph_target_0 = target_0['Petal Height'].var()
  var_pw_target_0 = target_0['Petal Width'].var()

  mean_sh_target_1 = target_1['Sepal Height'].mean() 
  mean_sw_target_1 = target_1['Sepal Width'].mean()
  mean_ph_target_1 = target_1['Petal Height'].mean()
  mean_pw_target_1 = target_1['Petal Width'].mean()

  var_sh_target_1 = target_1['Sepal Height'].var() 
  var_sw_target_1 = target_1['Sepal Width'].var()
  var_ph_target_1 = target_1['Petal Height'].var()
  var_pw_target_1 = target_1['Petal Width'].var()

  mean_sh_target_2 = target_2['Sepal Height'].mean() 
  mean_sw_target_2 = target_2['Sepal Width'].mean()
  mean_ph_target_2 = target_2['Petal Height'].mean()
  mean_pw_target_2 = target_2['Petal Width'].mean()

  var_sh_target_2 = target_2['Sepal Height'].var() 
  var_sw_target_2 = target_2['Sepal Width'].var()
  var_ph_target_2 = target_2['Petal Height'].var()
  var_pw_target_2 = target_2['Petal Width'].var()

  p_sh_given_t0= Gaussian_pdf(features[0], mean_sh_target_0, var_sh_target_0)
  p_sw_given_t0= Gaussian_pdf(features[1], mean_sw_target_0, var_sw_target_0)
  p_ph_given_t0= Gaussian_pdf(features[2], mean_ph_target_0, var_ph_target_0)
  p_pw_given_t0= Gaussian_pdf(features[3], mean_pw_target_0, var_pw_target_0)

  p_sh_given_t1= Gaussian_pdf(features[0], mean_sh_target_1, var_sh_target_1)
  p_sw_given_t1= Gaussian_pdf(features[1], mean_sw_target_1, var_sw_target_1)
  p_ph_given_t1= Gaussian_pdf(features[2], mean_ph_target_1, var_ph_target_1)
  p_pw_given_t1= Gaussian_pdf(features[3], mean_pw_target_1, var_pw_target_1)
  
  p_sh_given_t2= Gaussian_pdf(features[0], mean_sh_target_2, var_sh_target_2)
  p_sw_given_t2= Gaussian_pdf(features[1], mean_sw_target_2, var_sw_target_2)
  p_ph_given_t2= Gaussian_pdf(features[2], mean_ph_target_2, var_ph_target_2)
  p_pw_given_t2= Gaussian_pdf(features[3], mean_pw_target_2, var_pw_target_2)


  p0= p_sh_given_t0 * p_sw_given_t0 * p_ph_given_t0 * p_pw_given_t0
  p1= p_sh_given_t1 * p_sw_given_t1 * p_ph_given_t1 * p_pw_given_t1
  p2= p_sh_given_t2 * p_sw_given_t2 * p_ph_given_t2 * p_pw_given_t2

  
  return np.argmax([p0, p1, p2])

"""Test Naive Bayes with a prediction.

Get the corresponding class for a flower having the following features [4.9, 3.0,	1.4,	0.2].
"""

naive_bayes (df, [4.9, 3.0, 1.4, 0.2], 'Target')

"""See the performance of our NB model

Now here we will splot our data between 2 sets:

*   One from which the Naive Bayes Model will take the probabilities. (The **old** set) 80%
*   one that it hasn't seen before to test on it (The **new** set) 20%
"""

from sklearn.model_selection import train_test_split

X = df.values  
y = df.iloc[:,-1].values  

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""Now use the function you built and get the corresponding testing predictions, and then compute the accuracy of your model."""

y_test_predicted= []
features_test=x_test[:,0:-1]
x_test_df=pd.DataFrame(x_test)
x_test_df.columns = ['Sepal Height', 'Sepal Width', 'Petal Height', 'Petal Width', 'Target']
n_test=len(x_test)

for i in range (n_test):
  y_pred= naive_bayes (x_test_df, features_test[i,:], 'Target')
  y_test_predicted.append(y_pred)


y_wrong = np.count_nonzero(y_test - y_test_predicted, axis = 0)
y_correct = n_test - y_wrong
accuracy_score = float(y_correct)/n_test
print("Our Classifier has an accuracy score of ", accuracy_score*100, "%")